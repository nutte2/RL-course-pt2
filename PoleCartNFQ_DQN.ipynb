{"cells":[{"cell_type":"code","execution_count":1,"id":"aeec818f-b6bd-472d-88c5-20b5ced26a30","metadata":{"id":"aeec818f-b6bd-472d-88c5-20b5ced26a30","executionInfo":{"status":"ok","timestamp":1653397180515,"user_tz":-120,"elapsed":6,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","from matplotlib import image\n","import numpy as np\n","from collections import deque\n","import math\n","import random"]},{"cell_type":"code","execution_count":2,"id":"dc185502-2538-4e7c-9f3b-5c481d5b4d5b","metadata":{"id":"dc185502-2538-4e7c-9f3b-5c481d5b4d5b","executionInfo":{"status":"ok","timestamp":1653397188802,"user_tz":-120,"elapsed":3616,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda"]},{"cell_type":"code","execution_count":3,"id":"b7670f7a-545d-4852-a699-fa86b6d66bad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7670f7a-545d-4852-a699-fa86b6d66bad","outputId":"c2c22e48-fdc5-46a2-a3be-f26576e4c356","executionInfo":{"status":"ok","timestamp":1653397188803,"user_tz":-120,"elapsed":6,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","id":"405a78a0-40d7-45ec-b265-0ae2adf8291c","metadata":{"id":"405a78a0-40d7-45ec-b265-0ae2adf8291c"},"source":["# Implementing the NFQ algorithm\n","I've chosen the mountain car environment to try out the NFQ-algorithm\n","Some learning highlights:\n","* Hint-to-goal heuristic seems crucial for the success\n","* Make sure to have cost & min Q / reward max Q in all places.\n","* The number of episodes + increment fed to the training process in each batch seems to hav a large impact\n","* Continuing/stopping the training at the right time point is crucial. I've achieved good results with this implementation, but then ruined a good trained NFQ agent"]},{"cell_type":"code","execution_count":5,"id":"81dffc7c-02ef-4ab3-80c4-51d0fb9a580c","metadata":{"id":"81dffc7c-02ef-4ab3-80c4-51d0fb9a580c","executionInfo":{"status":"ok","timestamp":1653397215101,"user_tz":-120,"elapsed":262,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["import gym\n","# Set up environment and do a test run\n","env = gym.make('CartPole-v0')"]},{"cell_type":"code","source":["env.action_space\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahHlTYsrxRBp","outputId":"34b363ba-b1c7-4655-fbb0-992d69c1b5d8","executionInfo":{"status":"ok","timestamp":1653399985845,"user_tz":-120,"elapsed":236,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"id":"ahHlTYsrxRBp","execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(2)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":7,"id":"0654957c-ac37-4628-b72f-437a347e4c52","metadata":{"id":"0654957c-ac37-4628-b72f-437a347e4c52","executionInfo":{"status":"ok","timestamp":1653397221610,"user_tz":-120,"elapsed":253,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["if False:\n","    env.reset()\n","    n_transitions = 200\n","\n","    for i in range(n_transitions):\n","        state, reward, done, _ = env.step(env.action_space.sample())\n","        env.render()\n","        if done:\n","            env.reset()"]},{"cell_type":"code","execution_count":12,"id":"d60f2fbd-2104-4c3a-bb33-bc950215728d","metadata":{"id":"d60f2fbd-2104-4c3a-bb33-bc950215728d","executionInfo":{"status":"ok","timestamp":1653398700018,"user_tz":-120,"elapsed":264,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["class NFQLearningAgent():\n","    def __init__(self,env,discount_factor, nfq_net):\n","        self.env = env\n","        self.gamma = discount_factor\n","        self.nfq_net = nfq_net\n","        self.x_success_range = 2.4\n","        self.theta_success_range = 12 * 2 * math.pi / 360\n","   \n","    def run_one_episode(self):\n","        episode = []\n","        done = False\n","        total_cost = 0\n","        state = self.env.reset()\n","        \n","        for _ in range(200):\n","            # get best action from the NFQ net\n","            q_s = self.nfq_net.get_qs(state)\n","            action = np.argmin(q_s)\n","            new_state, reward, done, info = self.env.step(action) \n","            # Convert rewards to cost in the interval (0.0, 1,0)\n","            cost = 0.01\n","            if done and not ('TimeLimit.truncated' in info and info['TimeLimit.truncated']):\n","                cost = 0.0\n","            \n","            episode.append((state, action, cost, new_state, done, info))\n","            # Update state\n","            state = new_state\n","\n","            total_cost += cost\n","            if done:\n","                break\n","        return episode, total_cost\n","    \n","    def generate_experiences(self, n_episodes):\n","        experiences=[]\n","        for _ in range(n_episodes):\n","            episode, _ = self.run_one_episode()\n","            experiences.extend(episode)\n","        return experiences\n","    \n","    def get_goal_experience(self, size):\n","        # Hint-to-goal heuristic with values extracted from env\n","        goal_experiences = []\n","        \n","        for i in range(size):\n","            goal_experiences.append(np.array([np.random.uniform(-0.05, 0.05), \n","                                              np.random.normal(), \n","                                              np.random.uniform(-self.theta_success_range, self.theta_success_range), \n","                                              np.random.normal(), \n","                                              np.random.randint(2)]))\n","        goal_targets = np.zeros(size, dtype = np.float32)\n","        return np.array(goal_experiences), goal_targets\n","                            "]},{"cell_type":"code","execution_count":29,"id":"7f53c759-d605-4bf9-9ed6-cd6bcdf71a09","metadata":{"id":"7f53c759-d605-4bf9-9ed6-cd6bcdf71a09","executionInfo":{"status":"ok","timestamp":1653400033552,"user_tz":-120,"elapsed":234,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        # Copying the neural network from Riedmiller 2005\n","        self.MLP = nn.Sequential(\n","            nn.Linear(5, 5),\n","            nn.Sigmoid(),\n","            nn.Linear(5, 5),\n","            nn.Sigmoid(),\n","            nn.Linear(5, 5),\n","            nn.Sigmoid(),\n","            nn.Linear(5, 1),\n","        )\n","          # Initialize weights to [-0.5, 0.5]\n","        def init_weights(m):\n","            if type(m) == nn.Linear:\n","                torch.nn.init.uniform_(m.weight, -0.5, 0.5)\n","              \n","        self.MLP.apply(init_weights)\n","\n","    def forward(self, x):\n","        \n","        logits = self.MLP(x)\n","        return logits\n","    \n","    def get_qs(self, state):\n","        qs = np.array([self.MLP(torch.cat([torch.FloatTensor(state), torch.FloatTensor([i])], dim=0)).detach().numpy() for i in range(2)]).flatten()\n","        return qs"]},{"cell_type":"raw","id":"bff1cba1-6b3e-41db-ab6f-60ad04f0d4b8","metadata":{"id":"bff1cba1-6b3e-41db-ab6f-60ad04f0d4b8"},"source":["From Riedmiller 2005: This is the pseudo code I'm trying to implement\n","\n","NFQ main() {\n","input: a set of transition samples D; output: Q-value function QN\n","k=0\n","init MLP() → Q0;\n","Do {\n","generate pattern set P = {(inputl, targetl), l = 1, . . . ,#D} where:\n","input_l = sl, ul,\n","target_l = c(sl, ul, s\u0001l) +γ minb Qk(s\u0001l, b)\n","Rprop training(P) → Qk+1\n","k:= k+1\n","} While (k <N)"]},{"cell_type":"code","execution_count":30,"id":"008a3c5d-b969-4bff-ac95-38441b41d45c","metadata":{"id":"008a3c5d-b969-4bff-ac95-38441b41d45c","executionInfo":{"status":"ok","timestamp":1653400085924,"user_tz":-120,"elapsed":233,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["def generate_pattern_set(experiences, model, discount_factor):\n","    state_b, action_b, cost_b, next_state_b, done_b,_ = zip(*experiences)\n","    state_b = torch.FloatTensor(np.array(state_b))\n","    action_b = torch.FloatTensor(np.array(action_b))\n","    cost_b = torch.FloatTensor(np.array(cost_b))\n","    next_state_b = torch.FloatTensor(np.array(next_state_b))\n","    done_b = torch.FloatTensor(np.array(done_b))\n","\n","    # Create the input_l = sl, ul\n","    state_action_b = torch.cat([state_b, action_b.unsqueeze(1)], 1)\n","\n","    # Compute minb Qk(s\u0001l, b)\n","    q_next_state_0_b = model(torch.cat([next_state_b, torch.zeros(len(experiences), 1)], 1)).squeeze()\n","    q_next_state_1_b = model(torch.cat([next_state_b, torch.ones(len(experiences), 1)], 1)).squeeze()\n","    q_next_state_2_b = model(torch.cat([next_state_b, torch.full((len(experiences), 1), fill_value=2)], 1)).squeeze()\n","    q_next_state_b,_ = torch.min(torch.stack([q_next_state_0_b, q_next_state_1_b, q_next_state_2_b]),dim=0)\n","    \n","    # Create the target_l = c(sl, ul, s\u0001l) +γ minb Qk(s\u0001l, b)\n","    with torch.no_grad():\n","        target_q_values = cost_b + discount_factor * q_next_state_b * (1 - done_b)\n","\n","    return state_action_b, target_q_values\n","\n","def train_loop(agent, loss_fn, optimizer, discount_factor):\n","    losses=[]\n","    \n","    N_batches = 500\n","    state_actions = torch.empty(0,5)\n","    target_q_values = torch.empty(0)\n","    \n","    #Incrementally add one episode + 100 hint-to-goal heuristics for each batch\n","    for i_batch in range(N_batches):\n","        # Generating experiences\n","        experiences = agent.generate_experiences(1)\n","        # Extract and format state_actions and targets       \n","        state_actions_1, target_q_values_1 = generate_pattern_set(experiences, agent.nfq_net, discount_factor)\n","        state_actions = torch.cat([state_actions, state_actions_1], dim=0)\n","        target_q_values = torch.cat([target_q_values, target_q_values_1], dim=0)\n","\n","        # Add hint-to-goal heuristics with a factor of 100\n","        goal_state_action_b, goal_target_q_values = agent.get_goal_experience(100)\n","\n","        goal_state_action_b = torch.FloatTensor(goal_state_action_b)\n","        goal_target_q_values = torch.FloatTensor(goal_target_q_values)\n","        \n","        state_actions = torch.cat([state_actions, goal_state_action_b], dim=0)\n","        target_q_values = torch.cat([target_q_values, goal_target_q_values], dim=0)\n","            \n","        # Compute prediction and loss\n","        pred = model(state_actions).squeeze()\n","        \n","        loss = loss_fn(pred, target_q_values)\n","        losses.append(loss.item())\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    return losses\n","\n","\n","def test_loop(agent, n_trials):\n","    success = 0.\n","    for i in range (n_trials):\n","        episode, total_cost = agent.run_one_episode()\n","        done = episode[-1][-2]\n","        info = episode[-1][-1]\n","        if done and not ('TimeLimit.truncated' in info and info['TimeLimit.truncated']):\n","            success+=1.\n","        \n","    success /= n_trials\n","    print(f\"Test run, success: {(100*success):>0.1f}%\")\n","    return success*100"]},{"cell_type":"code","execution_count":31,"id":"2362a132-567f-4d10-962d-4feb7b5305a7","metadata":{"id":"2362a132-567f-4d10-962d-4feb7b5305a7","executionInfo":{"status":"ok","timestamp":1653400088280,"user_tz":-120,"elapsed":229,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[],"source":["model = NeuralNetwork()\n","gamma = 0.99\n","loss_fn=nn.MSELoss()\n","optimizer = torch.optim.Rprop(model.parameters())\n","\n","NFQagent = NFQLearningAgent(env,gamma, model)"]},{"cell_type":"code","execution_count":32,"id":"fc0591a9-56eb-43f6-a763-53f998bd61a7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc0591a9-56eb-43f6-a763-53f998bd61a7","outputId":"00734a6a-90dc-4d04-a4d9-d6bb097e912f","executionInfo":{"status":"ok","timestamp":1653400096563,"user_tz":-120,"elapsed":6138,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","Test run, success: 100.0%\n","Done!\n"]}],"source":["losses=[]\n","epochs = 20\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    losses.extend(train_loop(NFQagent, loss_fn, optimizer, gamma))\n","    success_rate = test_loop(NFQagent, 100)\n","    if (success_rate >= 99.0):\n","        break\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":33,"id":"573bdb6f-2785-4bb8-9f5a-5360fb3011c1","metadata":{"id":"573bdb6f-2785-4bb8-9f5a-5360fb3011c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653400137049,"user_tz":-120,"elapsed":1590,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}},"outputId":"69d6eddd-31fc-48f5-b6a3-c4f070caac1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test run, success: 100.0%\n"]},{"output_type":"execute_result","data":{"text/plain":["100.0"]},"metadata":{},"execution_count":33}],"source":["# Do a test run using the trained NFQ agent\n","test_loop(NFQagent, 500)"]},{"cell_type":"code","execution_count":34,"id":"f6ae22e9-5a37-4d0b-bba5-7adcf051b665","metadata":{"id":"f6ae22e9-5a37-4d0b-bba5-7adcf051b665","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"error","timestamp":1653400142410,"user_tz":-120,"elapsed":630,"user":{"displayName":"Peter Kimstrand","userId":"08323163514993465067"}},"outputId":"02188ba7-435b-48e0-d46b-7f4e91d15f76"},"outputs":[{"output_type":"error","ename":"NoSuchDisplayException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-9d6491acdc8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'TimeLimit.truncated'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""]}],"source":["# For visualization\n","state = env.reset()\n","n_transitions = 1000\n","\n","for i in range(n_transitions):\n","    q_s = np.array([NFQagent.nfq_net(torch.cat([torch.FloatTensor(state), torch.FloatTensor([i])], dim=0)).detach().numpy() for i in range(3)]).flatten()\n","    action = np.argmin(q_s)\n","    #print(action, q_s)\n","    state, reward, done, info = env.step(action)\n","    \n","    env.render()\n","    if done:\n","        if 'TimeLimit.truncated' in info:\n","            print(info)\n","        else:\n","            print('Terminated: Success')\n","        env.reset()"]},{"cell_type":"code","execution_count":null,"id":"928b545c-4901-4b6a-8ba3-9d187fe011af","metadata":{"id":"928b545c-4901-4b6a-8ba3-9d187fe011af"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"PoleCartNFQ_DQN.ipynb","provenance":[{"file_id":"https://github.com/pkimstrand/RL-course-pt2/blob/main/PoleCartNFQ_DQN.ipynb","timestamp":1653397147341}]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}