{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeec818f-b6bd-472d-88c5-20b5ced26a30",
      "metadata": {
        "id": "aeec818f-b6bd-472d-88c5-20b5ced26a30"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc185502-2538-4e7c-9f3b-5c481d5b4d5b",
      "metadata": {
        "id": "dc185502-2538-4e7c-9f3b-5c481d5b4d5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7670f7a-545d-4852-a699-fa86b6d66bad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7670f7a-545d-4852-a699-fa86b6d66bad",
        "outputId": "368786a7-07ba-4564-d137-f5fdb3127db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "405a78a0-40d7-45ec-b265-0ae2adf8291c",
      "metadata": {
        "id": "405a78a0-40d7-45ec-b265-0ae2adf8291c"
      },
      "source": [
        "# Implementing the NFQ algorithm\n",
        "I've chosen the mountain car environment to try out the NFQ-algorithm\n",
        "Some learning highlights:\n",
        "* Hint-to-goal heuristic seems crucial for the success\n",
        "* Make sure to have cost & min Q / reward max Q in all places.\n",
        "* The number of episodes + increment fed to the training process in each batch seems to hav a large impact\n",
        "* Continuing/stopping the training at the right time point is crucial. I've achieved good results with this implementation, but then ruined a good trained NFQ agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81dffc7c-02ef-4ab3-80c4-51d0fb9a580c",
      "metadata": {
        "id": "81dffc7c-02ef-4ab3-80c4-51d0fb9a580c"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "# Set up environment and do a test run\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahHlTYsrxRBp",
        "outputId": "a801709e-cfda-4fc8-db45-7cc20b585dfa"
      },
      "id": "ahHlTYsrxRBp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0654957c-ac37-4628-b72f-437a347e4c52",
      "metadata": {
        "id": "0654957c-ac37-4628-b72f-437a347e4c52"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    env.reset()\n",
        "    n_transitions = 200\n",
        "\n",
        "    for i in range(n_transitions):\n",
        "        state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        env.render()\n",
        "        if done:\n",
        "            env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60f2fbd-2104-4c3a-bb33-bc950215728d",
      "metadata": {
        "id": "d60f2fbd-2104-4c3a-bb33-bc950215728d"
      },
      "outputs": [],
      "source": [
        "class NFQLearningAgent():\n",
        "    def __init__(self,env,discount_factor, nfq_net):\n",
        "        self.env = env\n",
        "        self.gamma = discount_factor\n",
        "        self.nfq_net = nfq_net\n",
        "   \n",
        "    def run_one_episode(self):\n",
        "        episode = []\n",
        "        done = False\n",
        "        total_cost = 0\n",
        "        state = self.env.reset()\n",
        "        \n",
        "        for _ in range(200):\n",
        "            # get best action from the NFQ net\n",
        "            q_s = self.nfq_net.get_qs(state)\n",
        "            action = np.argmin(q_s)\n",
        "            new_state, reward, done, info = self.env.step(action) \n",
        "            # Convert rewards to cost in the interval (0.0, 1,0)\n",
        "            cost = 0.01\n",
        "            if done and not ('TimeLimit.truncated' in info and info['TimeLimit.truncated']):\n",
        "                cost = 0.0\n",
        "            \n",
        "            episode.append((state, action, cost, new_state, done, info))\n",
        "            # Update state\n",
        "            state = new_state\n",
        "\n",
        "            total_cost += cost\n",
        "            if done:\n",
        "                break\n",
        "        return episode, total_cost\n",
        "    \n",
        "    def generate_experiences(self, n_episodes):\n",
        "        experiences=[]\n",
        "        for _ in range(n_episodes):\n",
        "            episode, _ = self.run_one_episode()\n",
        "            experiences.extend(episode)\n",
        "        return experiences\n",
        "    \n",
        "    def get_goal_experience(self, size):\n",
        "        # Hint-to-goal heuristic with values extracted from env\n",
        "        goal_experiences = []\n",
        "        \n",
        "        for i in range(size):\n",
        "            goal_experiences.append(np.array([np.random.uniform(0.5, 0.6), np.random.uniform(-0.07, 0.07), np.random.randint(3)]))\n",
        "        goal_targets = np.zeros(size, dtype = np.float32)\n",
        "        return np.array(goal_experiences), goal_targets\n",
        "                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f53c759-d605-4bf9-9ed6-cd6bcdf71a09",
      "metadata": {
        "id": "7f53c759-d605-4bf9-9ed6-cd6bcdf71a09"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Copying the neural network from Riedmiller 2005\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(3, 5),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(5, 5),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(5, 5),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(5, 1),\n",
        "        )\n",
        "          # Initialize weights to [-0.5, 0.5]\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                torch.nn.init.uniform_(m.weight, -0.5, 0.5)\n",
        "              \n",
        "        self.MLP.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        logits = self.MLP(x)\n",
        "        return logits\n",
        "    \n",
        "    def get_qs(self, state):\n",
        "        qs = np.array([self.MLP(torch.cat([torch.FloatTensor(state), torch.FloatTensor([i])], dim=0)).detach().numpy() for i in range(3)]).flatten()\n",
        "        return qs"
      ]
    },
    {
      "cell_type": "raw",
      "id": "bff1cba1-6b3e-41db-ab6f-60ad04f0d4b8",
      "metadata": {
        "id": "bff1cba1-6b3e-41db-ab6f-60ad04f0d4b8"
      },
      "source": [
        "From Riedmiller 2005: This is the pseudo code I'm trying to implement\n",
        "\n",
        "NFQ main() {\n",
        "input: a set of transition samples D; output: Q-value function QN\n",
        "k=0\n",
        "init MLP() → Q0;\n",
        "Do {\n",
        "generate pattern set P = {(inputl, targetl), l = 1, . . . ,#D} where:\n",
        "input_l = sl, ul,\n",
        "target_l = c(sl, ul, s\u0001l) +γ minb Qk(s\u0001l, b)\n",
        "Rprop training(P) → Qk+1\n",
        "k:= k+1\n",
        "} While (k <N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008a3c5d-b969-4bff-ac95-38441b41d45c",
      "metadata": {
        "id": "008a3c5d-b969-4bff-ac95-38441b41d45c"
      },
      "outputs": [],
      "source": [
        "def generate_pattern_set(experiences, model, discount_factor):\n",
        "    state_b, action_b, cost_b, next_state_b, done_b,_ = zip(*experiences)\n",
        "    state_b = torch.FloatTensor(np.array(state_b))\n",
        "    action_b = torch.FloatTensor(np.array(action_b))\n",
        "    cost_b = torch.FloatTensor(np.array(cost_b))\n",
        "    next_state_b = torch.FloatTensor(np.array(next_state_b))\n",
        "    done_b = torch.FloatTensor(np.array(done_b))\n",
        "\n",
        "    # Create the input_l = sl, ul\n",
        "    state_action_b = torch.cat([state_b, action_b.unsqueeze(1)], 1)\n",
        "\n",
        "    # Compute minb Qk(s\u0001l, b)\n",
        "    q_next_state_0_b = model(torch.cat([next_state_b, torch.zeros(len(experiences), 1)], 1)).squeeze()\n",
        "    q_next_state_1_b = model(torch.cat([next_state_b, torch.ones(len(experiences), 1)], 1)).squeeze()\n",
        "    q_next_state_2_b = model(torch.cat([next_state_b, torch.full((len(experiences), 1), fill_value=2)], 1)).squeeze()\n",
        "    q_next_state_b,_ = torch.min(torch.stack([q_next_state_0_b, q_next_state_1_b, q_next_state_2_b]),dim=0)\n",
        "    \n",
        "    # Create the target_l = c(sl, ul, s\u0001l) +γ minb Qk(s\u0001l, b)\n",
        "    with torch.no_grad():\n",
        "        target_q_values = cost_b + discount_factor * q_next_state_b * (1 - done_b)\n",
        "\n",
        "    return state_action_b, target_q_values\n",
        "\n",
        "def train_loop(agent, loss_fn, optimizer, discount_factor):\n",
        "    losses=[]\n",
        "    \n",
        "    N_batches = 500\n",
        "    state_actions = torch.empty(0,3)\n",
        "    target_q_values = torch.empty(0)\n",
        "    \n",
        "    #Incrementally add one episode + 100 hint-to-goal heuristics for each batch\n",
        "    for i_batch in range(N_batches):\n",
        "        # Generating experiences\n",
        "        experiences = agent.generate_experiences(1)\n",
        "        # Extract and format state_actions and targets       \n",
        "        state_actions_1, target_q_values_1 = generate_pattern_set(experiences, agent.nfq_net, discount_factor)\n",
        "        state_actions = torch.cat([state_actions, state_actions_1], dim=0)\n",
        "        target_q_values = torch.cat([target_q_values, target_q_values_1], dim=0)\n",
        "\n",
        "        # Add hint-to-goal heuristics with a factor of 100\n",
        "        goal_state_action_b, goal_target_q_values = agent.get_goal_experience(100)\n",
        "\n",
        "        goal_state_action_b = torch.FloatTensor(goal_state_action_b)\n",
        "        goal_target_q_values = torch.FloatTensor(goal_target_q_values)\n",
        "        \n",
        "        state_actions = torch.cat([state_actions, goal_state_action_b], dim=0)\n",
        "        target_q_values = torch.cat([target_q_values, goal_target_q_values], dim=0)\n",
        "            \n",
        "        # Compute prediction and loss\n",
        "        pred = model(state_actions).squeeze()\n",
        "        \n",
        "        loss = loss_fn(pred, target_q_values)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def test_loop(agent, n_trials):\n",
        "    success = 0.\n",
        "    for i in range (n_trials):\n",
        "        episode, total_cost = agent.run_one_episode()\n",
        "        done = episode[-1][-2]\n",
        "        info = episode[-1][-1]\n",
        "        if done and not ('TimeLimit.truncated' in info and info['TimeLimit.truncated']):\n",
        "            success+=1.\n",
        "        \n",
        "    success /= n_trials\n",
        "    print(f\"Test run, success: {(100*success):>0.1f}%\")\n",
        "    return success*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2362a132-567f-4d10-962d-4feb7b5305a7",
      "metadata": {
        "id": "2362a132-567f-4d10-962d-4feb7b5305a7"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork()\n",
        "gamma = 0.99\n",
        "loss_fn=nn.MSELoss()\n",
        "optimizer = torch.optim.Rprop(model.parameters())\n",
        "\n",
        "NFQagent = NFQLearningAgent(env,gamma, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0591a9-56eb-43f6-a763-53f998bd61a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fc0591a9-56eb-43f6-a763-53f998bd61a7",
        "outputId": "ceb519c7-8f08-486a-8781-333e0ae7bcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test run, success: 0.0%\n",
            "Epoch 9\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3161\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'ndim'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-73d9468a0334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFQagent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msuccess_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFQagent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuccess_rate\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m99.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3b855023b62d>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(agent, loss_fn, optimizer, discount_factor)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Generating experiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Extract and format state_actions and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstate_actions_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_pattern_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfq_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4e805c21aa3e>\u001b[0m in \u001b[0;36mgenerate_experiences\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mexperiences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mexperiences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4e805c21aa3e>\u001b[0m in \u001b[0;36mrun_one_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mq_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfq_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Convert rewards to cost in the interval (0.0, 1,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mvelocity\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgravity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \"\"\"\n\u001b[0;32m-> 2115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_clip_dep_is_byte_swapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_clip_dep_is_scalar_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0musing_deprecated_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# guarded to protect circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3161\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "losses=[]\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    losses.extend(train_loop(NFQagent, loss_fn, optimizer, gamma))\n",
        "    success_rate = test_loop(NFQagent, 100)\n",
        "    if (success_rate >= 99.0):\n",
        "        break\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573bdb6f-2785-4bb8-9f5a-5360fb3011c1",
      "metadata": {
        "id": "573bdb6f-2785-4bb8-9f5a-5360fb3011c1"
      },
      "outputs": [],
      "source": [
        "# Do a test run using the trained NFQ agent\n",
        "test_loop(NFQagent, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ae22e9-5a37-4d0b-bba5-7adcf051b665",
      "metadata": {
        "id": "f6ae22e9-5a37-4d0b-bba5-7adcf051b665"
      },
      "outputs": [],
      "source": [
        "# For visualization\n",
        "state = env.reset()\n",
        "n_transitions = 1000\n",
        "\n",
        "for i in range(n_transitions):\n",
        "    q_s = np.array([NFQagent.nfq_net(torch.cat([torch.FloatTensor(state), torch.FloatTensor([i])], dim=0)).detach().numpy() for i in range(3)]).flatten()\n",
        "    action = np.argmin(q_s)\n",
        "    #print(action, q_s)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    \n",
        "    env.render()\n",
        "    if done:\n",
        "        if 'TimeLimit.truncated' in info:\n",
        "            print(info)\n",
        "        else:\n",
        "            print('Terminated: Success')\n",
        "        env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928b545c-4901-4b6a-8ba3-9d187fe011af",
      "metadata": {
        "id": "928b545c-4901-4b6a-8ba3-9d187fe011af"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MoutainCarNFQ.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}